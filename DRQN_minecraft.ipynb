{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minecraft Agent - Deep Reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work realised in collaboration with:\n",
    "\n",
    "- [Clément Romac](https://github.com/ClementRomac)\n",
    "- [Pierre Leroy](https://github.com/PierreLeroyBdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the work of [Arthur Juliani](https://github.com/awjuliani/DeepRL-Agents) and the [Malmo Project](https://github.com/Microsoft/malmo).\n",
    "Thanks to [Gym Minecraft](https://github.com/tambetm/gym-minecraft) & [Tensorflow](https://www.tensorflow.org/)\n",
    "\n",
    "Work realised with the [installation of gym-minecraft on Ubuntu](rgfedrgfdgfdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minecraft\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envrionnemental Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frame size is a useful variable, it allows us to decide which size our inputs are going to be.\n",
    "The values are very important, wider your frame will be and longer the training session is going to be, however you 'll give more information to your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()\n",
    "frame_size = [60,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XML modified : reward -0.1 for each action, 1000 for win & -1000 for lava\n",
    "env = gym.make('MinecraftBasic-v0')  \n",
    "env.init(start_minecraft = True, videoResolution = [frame_size[0], frame_size[1]], allowDiscreteMovement = [\"move\", \"turn\"]) #Movements modified to a faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_spec = str(env.mission_spec).replace('MissionSpec:\\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\" ?>\\n', '')\n",
    "tree = etree.fromstring(mission_spec)\n",
    "for elem in tree.iter():\n",
    "    if(elem.tag == '{http://ProjectMalmo.microsoft.com}Reward'):\n",
    "        attribs = elem.attrib.values()\n",
    "        if attribs[0] == 'found_goal':\n",
    "            win_reward = int(attribs[2])\n",
    "        if attribs[0] == 'out_of_time':\n",
    "            out_of_time_reward = int(attribs[2])\n",
    "    if(elem.tag == '{http://ProjectMalmo.microsoft.com}RewardForSendingCommand'):\n",
    "        step_reward = int(elem.attrib.values()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Definition ( Feed Forward )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward():\n",
    "    def __init__(self):\n",
    "        self.x = tf.placeholder(\"float\", [None, num_input])\n",
    "        \n",
    "        self.n_hidden_1 = 1024\n",
    "        self.n_hidden_2 = 1024\n",
    "        \n",
    "        self.weights = {\n",
    "            'h1': tf.Variable(tf.random_normal([num_input, self.n_hidden_1])),\n",
    "            'h2': tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2])),\n",
    "            'out': tf.Variable(tf.random_normal([self.n_hidden_2, num_classes]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden_1])),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden_2])),\n",
    "            'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "        }\n",
    "        \n",
    "        self.layer_1 = tf.add(tf.matmul(self.x, self.weights['h1']), self.biases['b1'])\n",
    "        self.layer_2 = tf.add(tf.matmul(self.layer_1, self.weights['h2']), self.biases['b2'])\n",
    "        \n",
    "        self.logits_layer = tf.matmul(self.layer_2, self.weights['out']) + self.biases['out']\n",
    "        \n",
    "        #Scale the output to improve training\n",
    "        self.Qout = tf.div(tf.subtract(self.logits_layer, tf.reduce_min(self.logits_layer)), tf.subtract(tf.reduce_max(self.logits_layer), tf.reduce_min(self.logits_layer)))\n",
    "        \n",
    "        #Indexes of the actions the network shall take\n",
    "        self.prediction = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        #Multiply our Q values by a OneHotEncoding to only take the chosen ones.\n",
    "        self.actions_onehot = tf.one_hot(self.actions, num_classes, dtype = tf.float32)\n",
    "        #So that Q's going to be the Q values choosen by the network\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis = 1)\n",
    "\n",
    "        #NextQ corresponds to the Q estimated by the Bellman Equation\n",
    "        self.nextQ = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "\n",
    "        #Scale the output to improve training\n",
    "        self.nextQ_scaled = tf.div(tf.subtract(self.nextQ, tf.reduce_min(self.nextQ)), tf.subtract(tf.reduce_max(self.nextQ), tf.reduce_min(self.nextQ)))\n",
    "\n",
    "        #The loss value coresponds to the difference between the two different Q values estimated\n",
    "        self.loss = tf.reduce_mean(tf.square(self.nextQ_scaled - self.Q))\n",
    "        \n",
    "        #Let's print the important informations\n",
    "        tf.summary.histogram(\"nextQ\", self.nextQ)\n",
    "        tf.summary.histogram(\"Q\", self.Q)\n",
    "        tf.summary.scalar(\"LOSS_FUNCTION\", self.loss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        #We would prefer the Adam Optimizer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Definition 2 ( Convolutional )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional():\n",
    "    def __init__(self):\n",
    "        self.x = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "        #Reshape the flatten data with 3 channels (RGB)\n",
    "        self.input_layer = tf.reshape(self.x, [-1, frame_size[1], frame_size[0], 3])\n",
    "\n",
    "        #Convolutional Layer 1\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs = self.input_layer,\n",
    "            filters = 16,\n",
    "            kernel_size = [6, 6],\n",
    "            strides=[1, 1],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu)\n",
    "        \n",
    "        self.pool1 = tf.layers.max_pooling2d(inputs = self.conv1, pool_size = [2, 2], strides = 2)\n",
    "\n",
    "        #Convolutional Layer 2\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs = self.pool1,\n",
    "            filters = 32,           \n",
    "            kernel_size = [4, 4],\n",
    "            strides = [1, 1],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu)\n",
    "        \n",
    "        #Pooling Layer #2\n",
    "        self.pool2 = tf.layers.max_pooling2d(inputs = self.conv2, pool_size = [2, 2], strides = 2)\n",
    "\n",
    "        #Convolutional Layer 3\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs = self.pool2,\n",
    "            filters = 64,\n",
    "            kernel_size = [4, 4],\n",
    "            strides = [1, 1],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu)\n",
    "        \n",
    "        #Pooling Layer #3\n",
    "        self.pool3 = tf.layers.max_pooling2d(inputs = self.conv3, pool_size = [2, 2], strides = 2)\n",
    "\n",
    "        #Flatten the data to pass it through the feed forward\n",
    "        self.dims = self.pool3.get_shape().as_list()\n",
    "        self.final_dimension = self.dims[1] * self.dims[2] * self.dims[3]\n",
    "        self.conv3_flat = tf.reshape(self.pool3, [-1, self.final_dimension])\n",
    "        \n",
    "        #Feed Forward\n",
    "        self.dense = tf.layers.dense(inputs = self.conv3_flat, units = 512, activation = tf.nn.relu)\n",
    "        \n",
    "        self.logits_layer = tf.layers.dense(inputs = self.dense, units = num_classes)\n",
    "        \n",
    "        #Scale the output to improve training\n",
    "        self.Qout = tf.div(tf.subtract(self.logits_layer, tf.reduce_mean(self.logits_layer)), tf.subtract(tf.reduce_max(self.logits_layer), tf.reduce_min(self.logits_layer)))\n",
    "        \n",
    "        #Indexes of the actions the network shall take\n",
    "        self.prediction = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        #Multiply our Q values by a OneHotEncoding to only take the chosen ones.\n",
    "        self.actions_onehot = tf.one_hot(self.actions, num_classes, dtype = tf.float32)\n",
    "        #So that Q's going to be the Q values choosen by the network\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis = 1)\n",
    "\n",
    "        #NextQ corresponds to the Q estimated by the Bellman Equation\n",
    "        self.nextQ = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "\n",
    "        #Scale the output to improve training\n",
    "        self.nextQ_scaled = tf.div(tf.subtract(self.nextQ, tf.reduce_mean(self.nextQ)), tf.subtract(tf.reduce_max(self.nextQ), tf.reduce_min(self.nextQ)))\n",
    "                \n",
    "        #The loss value coresponds to the difference between the two different Q values estimated\n",
    "        self.loss = tf.reduce_mean(tf.square(self.nextQ_scaled - self.Q))\n",
    "        \n",
    "        #Let's print the important informations\n",
    "        #tf.summary.histogram(\"nextQ\", self.nextQ)\n",
    "        #tf.summary.histogram(\"Q\", self.Q)\n",
    "        tf.summary.scalar(\"LOSS_FUNCTION\", self.loss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        #We would prefer the Adam Optimizer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Definition 3 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, rnn_cell, scope):\n",
    "        self.x = tf.placeholder(\"float\", [None, num_input])\n",
    "        #Length of the frames' sequence \n",
    "        self.train_length = tf.placeholder(dtype=tf.int32)\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32,shape=[])\n",
    "\n",
    "        #Reshape the flatten data with 3 channels (RGB)\n",
    "        self.input_layer = tf.reshape(self.x, [-1, frame_size[1], frame_size[0], 3])\n",
    "\n",
    "        #Convolutional Layer 1\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs = self.input_layer,\n",
    "            filters = 32,\n",
    "            kernel_size = [6, 6],\n",
    "            strides=[2, 2],\n",
    "            padding = \"valid\",\n",
    "            activation = tf.nn.relu)\n",
    "        #Output size = 18\n",
    "\n",
    "        #Convolutional Layer 2\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs = self.conv1,\n",
    "            filters = 64,           \n",
    "            kernel_size = [4, 4],\n",
    "            strides = [2, 2],\n",
    "            padding = \"valid\",\n",
    "            activation = tf.nn.relu)\n",
    "        #Output size = 8\n",
    "\n",
    "        #Convolutional Layer 3\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs = self.conv2,\n",
    "            filters = num_nodes,#depth of the LSTM\n",
    "            kernel_size = [8, 8],\n",
    "            strides = [1, 1],\n",
    "            padding = \"valid\",\n",
    "            activation = tf.nn.relu)\n",
    "        #Output size = 1\n",
    "\n",
    "        #Flatten the data to pass it through the feed forward\n",
    "        self.dims = self.conv3.get_shape().as_list()\n",
    "        self.final_dimension = self.dims[1] * self.dims[2] * self.dims[3]\n",
    "        self.conv3_flat = tf.reshape(self.conv3, [-1, self.final_dimension])\n",
    "        self.rnn_input = tf.reshape(self.conv3_flat, [self.batch_size, self.train_length, num_nodes])\n",
    "        \n",
    "        #Initialize the LSTM state\n",
    "        self.lstm_state_in = rnn_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.rnn,self.rnn_state = tf.nn.dynamic_rnn(\\\n",
    "                inputs=self.rnn_input, cell=rnn_cell, dtype=tf.float32, initial_state=self.lstm_state_in, scope=scope + \"_rnn\")\n",
    "        self.rnn = tf.reshape(self.rnn,shape=[-1,num_nodes])\n",
    "        \n",
    "        #Feed Forward\n",
    "        self.dense = tf.layers.dense(inputs = self.rnn, units = 512, activation = tf.nn.relu)\n",
    "        \n",
    "        self.logits_layer = tf.layers.dense(inputs = self.dense, units = num_classes)\n",
    "        \n",
    "        #Scale the output to improve training\n",
    "        self.Qout = tf.div(tf.subtract(self.logits_layer, tf.reduce_mean(self.logits_layer)), tf.subtract(tf.reduce_max(self.logits_layer), tf.reduce_min(self.logits_layer)))\n",
    "        \n",
    "        #Indexes of the actions the network shall take\n",
    "        self.prediction = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.actions = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        #Multiply our actions values by a OneHotEncoding to only take the chosen ones.\n",
    "        self.actions_onehot = tf.one_hot(self.actions, num_classes, dtype = tf.float32)\n",
    "        #So that Q's going to be the Q values choosen by the network\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis = 1)\n",
    "\n",
    "        #NextQ corresponds to the Q estimated by the Bellman Equation\n",
    "        self.nextQ = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "\n",
    "        #Scale the output to improve training\n",
    "        self.nextQ_scaled = tf.div(tf.subtract(self.nextQ, tf.reduce_mean(self.nextQ)), tf.subtract(tf.reduce_max(self.nextQ), tf.reduce_min(self.nextQ)))\n",
    "                \n",
    "        #The loss value coresponds to the difference between the two different Q values estimated\n",
    "        self.loss = tf.reduce_mean(tf.square(self.nextQ_scaled - self.Q))\n",
    "        \n",
    "        #Let's print the important informations\n",
    "        tf.summary.histogram(\"nextQ\", self.nextQ)\n",
    "        tf.summary.histogram(\"Q\", self.Q)\n",
    "        tf.summary.scalar(\"LOSS_FUNCTION\", self.loss)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        #We would prefer the Adam Optimizer\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the Network\n",
    "Obtained results: \n",
    "- Simple Feed Forward : Loss > e¹⁶\n",
    "- Convolutional : Loss < e³\n",
    "- LSTM : Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChooseNetwork(Enum):\n",
    "    Feed_Forward = FeedForward\n",
    "    Convolutional = Convolutional\n",
    "    LSTM = LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetType = ChooseNetwork.Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.0001\n",
    "num_input = frame_size[0] * frame_size[1] * 3\n",
    "num_classes = len(env.action_names[0])\n",
    "num_nodes = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experience allows us to define the moves to take care for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])\n",
    "    \n",
    "    def get(self):\n",
    "        return np.reshape(np.array(self.buffer), [len(self.buffer), 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurrent_experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self,batch_size,trace_length):\n",
    "        tmp_buffer = [episode for episode in self.buffer if len(episode)+1>trace_length]\n",
    "        sampled_episodes = random.sample(tmp_buffer, batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0,len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces,[batch_size*trace_length,5])\n",
    "    \n",
    "    def get(self):\n",
    "        return np.reshape(np.array(self.buffer), [len(buffer), 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape all the data (for the feed forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states, [(num_input)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  32 #How many experiences to use for each training step.\n",
    "trace_length = 2 #How long each experience trace will be when training\n",
    "if NetType == ChooseNetwork.LSTM:\n",
    "    myBuffer = recurrent_experience_buffer()\n",
    "    batch_size = batch_size / trace_length\n",
    "else:\n",
    "    myBuffer = experience_buffer()\n",
    "    \n",
    "update_freq = 4 #How often to perform a training step.\n",
    "num_episodes = 1250000 #How many episodes of game environment to train network with\n",
    "num_steps = 100\n",
    "total_steps = 0\n",
    "rList = [] #List of our rewards gained by game\n",
    "jList = [] #Number of moves realised by game\n",
    "j_by_loss = [] #Number of moves before resulting with a death of the agent\n",
    "j_by_win = [] #Number of moves before resulting with a win of the agent\n",
    "j_by_nothing = [] #This list's going to be used to count how many times the agent move until the limit of moves\n",
    "y = .95 #Discount factor on the target Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exploration step is realized before any exploitation step during the training which allows us to obtain a set of data to work with.\n",
    "\n",
    "This exploration step reduces as the number of training increases, making the exploitation majority step by step (e-greedy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_episodes = 1250#5000 #How many episodes of random actions before training begins.\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_episodes = 25000#50000. #How many epsiodes of training to reduce startE to endE.\n",
    "e = startE\n",
    "stepDrop = (startE - endE) / annealing_episodes\n",
    "nb_win = 0\n",
    "nb_nothing = 0\n",
    "nb_loss = 0\n",
    "tau = 0.001\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an explicit and unique title for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(time.time()).replace(\".\",\"\")\n",
    "bs = \"BatchSize:\" + str(batch_size)\n",
    "strlr = \"lr:\" + str(learningRate)\n",
    "rand_step = \"RandStep:\" + str(pre_train_episodes)\n",
    "nb_to_reduce_e = \"ReducE:\" + str(annealing_episodes)\n",
    "write_path = \"./train/\" + bs + \"_\" + strlr + \"_\" + rand_step + \"_\" + nb_to_reduce_e + \"_\" + date[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #Use a Double Network\n",
    "    #Using a double network increases his strength\n",
    "    #Deep Reinforcement Learning with Double Q-learning : Hado van Hasselt and Arthur Guez and David Silver\n",
    "    if NetType == ChooseNetwork.LSTM:\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units = num_nodes, state_is_tuple = True)\n",
    "        cellT = tf.contrib.rnn.BasicLSTMCell(num_units = num_nodes, state_is_tuple = True)\n",
    "        \n",
    "        #Use a Double Network more robust\n",
    "        mainQN = NetType.value(cell, 'main')\n",
    "        targetQN = NetType.value(cellT, 'target')\n",
    "    else:\n",
    "        mainQN = NetType.value()\n",
    "        targetQN = NetType.value()\n",
    "    \n",
    "    trainables = tf.trainable_variables()\n",
    "\n",
    "    targetOps = updateTargetGraph(trainables,tau)\n",
    "    \n",
    "    #Save the network\n",
    "    saver = tf.train.Saver()\n",
    "    path_to_save = \"./saves/\" + str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) + \"/\"\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(write_path)\n",
    "    \n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path_to_save)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        if NetType == ChooseNetwork.LSTM:\n",
    "            episodeBuffer = recurrent_experience_buffer()\n",
    "            lstm_state = (np.zeros([1, num_nodes]), np.zeros([1, num_nodes])) #Reset the recurrent layer's hidden state\n",
    "        else:\n",
    "            episodeBuffer = experience_buffer()\n",
    "            \n",
    "        s = env.reset()\n",
    "        #env.render(mode=\"human\")\n",
    "        s = processState(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        episode_frames = []\n",
    "        episode_qvalues = []\n",
    "        \n",
    "        if i > pre_train_episodes:\n",
    "            #Reducing portion of exploration\n",
    "            if e > endE:\n",
    "                e -= stepDrop\n",
    "        \n",
    "        ZPos = []\n",
    "        XPos = []\n",
    "        Yaw = []\n",
    "        moves = []\n",
    "        \n",
    "        while not d:     \n",
    "            j += 1\n",
    "            #Make full exploration before the number or pre-train-episodes and play with an e chance of random action during the training (e-greedy)\n",
    "            if(np.random.rand(1) < e or i < pre_train_episodes):\n",
    "                if NetType == ChooseNetwork.LSTM:\n",
    "                    lstm_state1 = sess.run(mainQN.rnn_state,\\\n",
    "                        feed_dict = {mainQN.x:[s/255.0], mainQN.train_length:1, \\\n",
    "                                     mainQN.lstm_state_in:lstm_state, mainQN.batch_size:1})\n",
    "                index_action_predicted = env.action_space.sample()\n",
    "                episode_qvalues.append([1 if i == index_action_predicted else 0 for i in range(len(env.action_names[0]))])\n",
    "            else:\n",
    "                if NetType == ChooseNetwork.LSTM:\n",
    "                    prediction, qvalues, lstm_state1 = sess.run([mainQN.prediction, mainQN.Qout, mainQN.rnn_state],\\\n",
    "                        feed_dict={mainQN.x:[s/255.0], mainQN.train_length:1, \\\n",
    "                                   mainQN.lstm_state_in:lstm_state, mainQN.batch_size:1})\n",
    "                else:      \n",
    "                    prediction, qvalues = sess.run([mainQN.prediction, mainQN.Qout], \\\n",
    "                                                      feed_dict = {mainQN.x:[s/255.0]})\n",
    "                index_action_predicted = prediction[0]\n",
    "                episode_qvalues.append(qvalues[0])\n",
    "            \n",
    "            #Get new state and reward from environment\n",
    "            s1_raw, r, d, info = env.step(index_action_predicted)\n",
    "            if info[\"observation\"]:\n",
    "                ZPos.append(info['observation']['ZPos'])\n",
    "                XPos.append(info['observation']['XPos'])\n",
    "                Yaw.append(info['observation']['Yaw'])\n",
    "            s1 = processState(s1_raw)\n",
    "            moves.append(index_action_predicted)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s, index_action_predicted, r, s1, d]), [1, 5]))\n",
    "            episode_frames.append(s1_raw)\n",
    "            \n",
    "            if i > pre_train_episodes:\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    updateTarget(targetOps,sess)\n",
    "                    \n",
    "                    if NetType == ChooseNetwork.LSTM:\n",
    "                        lstm_state_train = (np.zeros([batch_size, num_nodes]), \\\n",
    "                                            np.zeros([batch_size, num_nodes]))\n",
    "                        \n",
    "                        trainBatch = myBuffer.sample(batch_size,trace_length)\n",
    "                        \n",
    "                        #Estimate the action to chose by our first network\n",
    "                        actionChosen = sess.run(mainQN.prediction, \\\n",
    "                                                feed_dict = {mainQN.x:np.vstack(trainBatch[:, 3]/255.0), \\\n",
    "                                                             mainQN.train_length:trace_length, \\\n",
    "                                                             mainQN.lstm_state_in:lstm_state_train, \\\n",
    "                                                             mainQN.batch_size:batch_size})\n",
    "                        #Estimate all the Q values by our second network --> Double\n",
    "                        allQValues = sess.run(targetQN.Qout, \\\n",
    "                                              feed_dict = {targetQN.x:np.vstack(trainBatch[:, 3]/255.0), \\\n",
    "                                                           targetQN.train_length:trace_length, \\\n",
    "                                                           targetQN.lstm_state_in:lstm_state_train, \\\n",
    "                                                           targetQN.batch_size:batch_size})\n",
    "\n",
    "                        #Train our network using target and predicted Q values\n",
    "                        end_multiplier = -(trainBatch[:, 4] -1)\n",
    "                        maxQ = allQValues[range(batch_size*trace_length), actionChosen]\n",
    "                        #Bellman Equation\n",
    "                        targetQ = trainBatch[:, 2] + (y * maxQ * end_multiplier)\n",
    "\n",
    "                        _, summaryPlot = sess.run([mainQN.updateModel, mainQN.merged], \\\n",
    "                                                  feed_dict = {mainQN.x:np.vstack(trainBatch[:, 0]/255.0), \\\n",
    "                                                               mainQN.nextQ:targetQ, \\\n",
    "                                                               mainQN.actions:trainBatch[:, 1], \\\n",
    "                                                               mainQN.train_length:trace_length, \\\n",
    "                                                               mainQN.lstm_state_in:lstm_state_train, \\\n",
    "                                                               mainQN.batch_size:batch_size})\n",
    "                        \n",
    "                        writer.add_summary(summaryPlot, total_steps)  \n",
    "                    else:\n",
    "                        trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "                    \n",
    "                        #Estimate the action to chose by our first network\n",
    "                        actionChosen = sess.run(mainQN.prediction, \\\n",
    "                                                feed_dict = {mainQN.x:np.vstack(trainBatch[:, 3]/255.0)})\n",
    "                        #Estimate all the Q values by our second network --> Double\n",
    "                        allQValues = sess.run(targetQN.Qout, \\\n",
    "                                              feed_dict = {targetQN.x:np.vstack(trainBatch[:, 3]/255.0)})\n",
    "\n",
    "                        #Train our network using target and predicted Q values\n",
    "                        end_multiplier = -(trainBatch[:, 4] -1)\n",
    "                        maxQ = allQValues[range(batch_size), actionChosen]\n",
    "                        #Bellman Equation\n",
    "                        targetQ = trainBatch[:, 2] + (y * maxQ * end_multiplier)\n",
    "\n",
    "                        _, summaryPlot = sess.run([mainQN.updateModel, mainQN.merged], \\\n",
    "                                                  feed_dict = {mainQN.x:np.vstack(trainBatch[:, 0]), \\\n",
    "                                                               mainQN.nextQ:targetQ, \\\n",
    "                                                               mainQN.actions:trainBatch[:, 1]})\n",
    "                        \n",
    "                        writer.add_summary(summaryPlot, total_steps)  \n",
    "                    \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if NetType == ChooseNetwork.LSTM:\n",
    "                lstm_state = lstm_state1\n",
    "                \n",
    "            if d == True:\n",
    "                if r == (win_reward+step_reward):\n",
    "                    j_by_win.append(j)\n",
    "                else:\n",
    "                    if r == (out_of_time_reward+step_reward):\n",
    "                        j_by_nothing.append(j)\n",
    "                    else:\n",
    "                        j_by_loss.append(j)\n",
    "                break\n",
    "                  \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        rewards = np.array(rList)\n",
    "        if i % (500) == 0:\n",
    "            print(\"#######################################\")\n",
    "            print(\"% Win : \" + str((len(j_by_win) - nb_win)/5) + \"%\")\n",
    "            print(\"% Nothing : \" + str((len(j_by_nothing) - nb_nothing)/5) + \"%\")\n",
    "            print(\"% Loss : \" + str((len(j_by_loss) - nb_loss)/5) + \"%\")\n",
    "            \n",
    "            print(\"Nb J before win: \" + str(np.mean(j_by_win[-(len(j_by_win) - nb_win):])))\n",
    "            print(\"Nb J before die: \" + str(np.mean(j_by_loss[-(len(j_by_loss) - nb_loss):])))\n",
    "                  \n",
    "            print(\"Total Steps: \" + str(total_steps))\n",
    "            print(\"I: \" + str(i))\n",
    "            print(\"Epsilon: \", str(e))\n",
    "                  \n",
    "            nb_win = len(j_by_win)\n",
    "            nb_nothing = len(j_by_nothing)\n",
    "            nb_loss = len(j_by_loss)\n",
    "            \n",
    "            print(\"#### LAST EPISODE MOVES ####\")\n",
    "            last_episode_moves = episodeBuffer.get()\n",
    "            for z in range(j):\n",
    "                if z < 5:    \n",
    "                    print(\"-----------------------\")\n",
    "                    plt.imshow(episode_frames[z])\n",
    "                    plt.show()\n",
    "\n",
    "                    print(\"- Buffer Move \" + str(z) + \" : \" + env.action_names[0][last_episode_moves[z, 1]])\n",
    "                    print(\"- Move Array \" + str(z) + \" : \" + env.action_names[0][moves[z]])\n",
    "                    if z != j-1:\n",
    "                        print(\"ZPos : \"+ str(ZPos[z]))\n",
    "                        print(\"XPos : \"+ str(XPos[z]))\n",
    "                        print(\"Yaw : \"+ str(Yaw[z]))\n",
    "                    figure = plt.figure()\n",
    "                    axes = figure.add_subplot(2, 1, 1)\n",
    "                    axes.matshow([episode_qvalues[z]])\n",
    "                    axes.set_xticks(range(len(env.action_names[0])))\n",
    "                    actions_names = [\"Straight\", \"Back\", \"Right\", \"Left\"]\n",
    "                    axes.set_xticklabels(actions_names)\n",
    "                    plt.show()\n",
    "\n",
    "                    print(\"         \" + \"          \".join(str(qval) for qval in episode_qvalues[z]))\n",
    "                \n",
    "        if i % (5000)== 0 and i != 0:\n",
    "            #Save all the other important values\n",
    "            saver.save(sess, path_to_save + str(i) + '.ckpt')\n",
    "            with open(path_to_save + str(i) + \".pickle\", 'wb') as file:\n",
    "                dictionnary = {\n",
    "                    \"epsilon\": e,\n",
    "                    \"Total_steps\": total_steps,\n",
    "                    \"Buffer\": myBuffer,\n",
    "                    \"rAll\": rAll,\n",
    "                    \"rList\": rList,\n",
    "                    \"Num Episodes\": i,\n",
    "                    \"jList\": jList\n",
    "                }\n",
    "                \n",
    "                pickle.dump(dictionnary, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    saver.save(sess, path_to_save + str(i) + '.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if NetType == ChooseNetwork.LSTM:\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units = num_nodes, state_is_tuple = True)\n",
    "        mainQN = NetType.value(cell, 'main')\n",
    "    else:\n",
    "        mainQN = NetType.value()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    path_to_save = \"./saves/_save_date/\"\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    print('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(path_to_save)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            \n",
    "    s = env.reset()\n",
    "    s = processState(s)\n",
    "    d = False\n",
    "    j = 0\n",
    "\n",
    "    while not d:  \n",
    "        env.render(mode=\"human\")\n",
    "        j += 1\n",
    "        if NetType == ChooseNetwork.LSTM:\n",
    "            prediction, qvalues, lstm_state1 = sess.run([mainQN.prediction, mainQN.Qout, mainQN.rnn_state],\\\n",
    "                feed_dict={mainQN.x:[s/255.0], mainQN.train_length:1, \\\n",
    "                           mainQN.lstm_state_in:lstm_state, mainQN.batch_size:1})\n",
    "        else:      \n",
    "            prediction, qvalues = sess.run([mainQN.prediction, mainQN.Qout], \\\n",
    "                                              feed_dict = {mainQN.x:[s/255.0]})\n",
    "        index_action_predicted = prediction[0]\n",
    "\n",
    "        #Get new state and reward from environment\n",
    "        s1_raw, r, d, info = env.step(index_action_predicted)\n",
    "\n",
    "        s1 = processState(s1_raw)\n",
    "        s = s1\n",
    "        if NetType == ChooseNetwork.LSTM:\n",
    "            lstm_state = lstm_state1\n",
    "\n",
    "        if d == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "- [Deep Recurrent Q-Learning for Partially Observable MDPs - Matthew Hausknecht and Peter Stone](https://arxiv.org/pdf/1507.06527.pdf)\n",
    "- [Deep Reinforcement Learning with Double Q-learning - Hado van Hasselt and Arthur Guez and David Silver](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "- [Teacher-Student Curriculum Learning - Tambet Matiisen and Avital Oliver and Taco Cohen and John Schulman](https://arxiv.org/pdf/1707.00183.pdf)\n",
    "- [Deep Learning for Video Game Playing - Niels Justesen and Philip Bontrager and Julian Togelius and Sebastian Risi](https://arxiv.org/pdf/1708.07902.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for reading ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
